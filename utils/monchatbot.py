import random  # Pour les r√©actions al√©atoires
import torch  # Torch, probablement utilis√© dans le mod√®le (import√© mais pas utilis√© directement ici)
from sentence_transformers import util  # Pour calculer la similarit√© cosinus entre vecteurs
from utils.wikipedia_search import get_wikipedia_summary  # Recherche et r√©sum√© Wikipedia
from utils.google_search import recherche_google  # Recherche Google via API
from utils.neural_net import model  # Mod√®le d'encodage de phrases
from app.config import WIKI_TRIGGER, TAILLE_MAX  # Constantes de configuration
from .neogpt import NeoGPT  # Classe NeoGPT pour g√©n√©ration de texte

# Initialisation d'une instance NeoGPT avec personnalit√© "assistant"
neo_gpt = NeoGPT(personality="assistant")

# Variable globale pour stocker les embeddings des questions en m√©moire
corpus_embeddings = None

def update_corpus_embeddings():
    from app.memory import memoire_cache, lock  # Import local pour √©viter import circulaire
    global corpus_embeddings
    with lock:  # S√©curise l'acc√®s concurrent aux donn√©es partag√©es
        questions = [item["question"] for item in memoire_cache]  # Extraction des questions en m√©moire
        # Encode les questions en vecteurs si la m√©moire n'est pas vide, sinon None
        corpus_embeddings = model.encode(questions, convert_to_tensor=True) if questions else None

# Mise √† jour initiale des embeddings √† partir de la m√©moire existante
update_corpus_embeddings()

def ton_humain_reponse(texte: str) -> str:
    # Pr√©pare une r√©ponse avec un pr√©fixe humain sympathique al√©atoire
    r√©actions = (
        ["üòä", "üëç", "√áa me fait plaisir de t'aider !", "Super question !", "Tu es brillant(e) !"],
        ["Hmm...", "Int√©ressant...", "Voyons voir...", "C'est une bonne question.", "Je r√©fl√©chis..."],
        ["Je ne suis pas une boule de cristal, mais je crois que c'est √ßa ! üòÇ",
         "Si j'avais un euro √† chaque fois qu'on me pose cette question... üí∏",
         "Je suis un bot, mais je commence √† comprendre les humains ! ü§ñ",
         "Je suis pas parfait, mais j'essaie ! üòÖ"]
    )
    r = random.random()  # Tirage al√©atoire
    # Choisit le groupe de r√©actions selon la probabilit√©
    prefix = random.choice(
        r√©actions[0] if r < 0.15 else r√©actions[2] if r < 0.2 else r√©actions[1]
    )
    return f"{prefix} {texte}"  # Concat√®ne le pr√©fixe avec la r√©ponse donn√©e

def detect_salutation(message: str) -> str | None:
    # D√©tecte si le message est une salutation ou une question sur l'√©tat et r√©pond
    msg = message.lower()
    if any(greet in msg for greet in ["bonjour", "salut", "coucou", "hello", "hey"]):
        return random.choice([
            "Bonjour ! Comment puis-je t'aider aujourd'hui ?",
            "Salut ! Ravi de te voir.",
            "Coucou ! Que puis-je faire pour toi ?"
        ])
    if any(x in msg for x in ["comment vas-tu", "comment √ßa va", "√ßa va", "tu vas bien"]):
        return random.choice([
            "Je vais tr√®s bien, merci ! Et toi ?",
            "Tout roule ici, pr√™t √† t'aider !",
            "Super bien, merci de demander !"
        ])
    return None  # Pas de salutation d√©tect√©e

def ajouter_a_memoire(question: str, reponse: str):
    from app.memory import memoire_cache, lock, save_memory  # Import local pour √©viter import circulaire
    with lock:  # Protection acc√®s m√©moire partag√©e
        memoire_cache.append({"question": question, "response": reponse})  # Ajout √† la m√©moire
        if len(memoire_cache) > TAILLE_MAX:  # Si d√©passe taille max, supprime la plus ancienne
            memoire_cache.pop(0)
        save_memory()  # Sauvegarde persistante de la m√©moire
        update_corpus_embeddings()  # Met √† jour les embeddings pour la recherche rapide

def obtenir_la_response(message: str) -> str:
    from app.memory import memoire_cache, lock  # Import local m√©moire et verrou

    msg = message.strip()  # Nettoyage du message
    if not msg:
        return "Je n'ai pas bien saisi ta question, pourrais-tu reformuler s'il te pla√Æt ?"

    if (resp := detect_salutation(msg)):  # V√©rifie si c'est une salutation
        return resp

    if msg.lower().startswith(WIKI_TRIGGER):  # D√©tecte la commande de recherche Wikipedia
        query = msg[len(WIKI_TRIGGER):].strip()
        if not query:
            return "Tu dois me dire ce que tu veux que je cherche sur Wikip√©dia."
        try:
            if (res := get_wikipedia_summary(query)):  # Recherche r√©sum√© Wikipedia
                return ton_humain_reponse(f"Voici ce que j'ai trouv√© sur Wikip√©dia :\n{res}")
            return ton_humain_reponse("D√©sol√©, rien trouv√© de pertinent sur Wikip√©dia.")
        except Exception as e:
            return ton_humain_reponse(f"Erreur lors de la recherche Wikip√©dia : {e}")

    try:
        embedding = model.encode(msg, convert_to_tensor=True)  # Encode la question en vecteur
    except Exception as e:
        return ton_humain_reponse(f"Erreur lors de l'encodage de la question : {e}")

    global corpus_embeddings
    with lock:
        if not memoire_cache:  # Si la m√©moire est vide, on ajoute et r√©pond
            ajouter_a_memoire(msg, "Je vais m'en souvenir pour la prochaine fois.")
            return ton_humain_reponse(f"C'est la premi√®re fois que tu me poses √ßa, je retiens : ¬´ {msg} ¬ª")

        if corpus_embeddings is not None:
            try:
                scores = util.cos_sim(embedding, corpus_embeddings)[0]  # Calcul similarit√© cosinus
                idx = int(scores.argmax())  # Index de la meilleure correspondance
                max_score = float(scores[idx])

                # Seuil variable selon taille de m√©moire pour accepter correspondance
                seuil = 0.55 if len(memoire_cache) < 10 else 0.60 if len(memoire_cache) < 50 else 0.65
                if max_score >= seuil:
                    # Retourne la r√©ponse associ√©e √† la question la plus proche
                    return ton_humain_reponse(f"Je pense que ceci r√©pond √† ta question :\n{memoire_cache[idx]['response']}")
            except Exception as e:
                return ton_humain_reponse(f"Erreur lors de la recherche dans la m√©moire : {e}")

    try:
        res = neo_gpt.chat(msg, max_length=100)  # G√©n√®re une r√©ponse via NeoGPT
        ajouter_a_memoire(msg, res)  # Ajoute la nouvelle question/r√©ponse en m√©moire
        return ton_humain_reponse(res)
    except Exception as e:
        try:
            if (res := recherche_google(msg)):  # En cas d'√©chec, tente recherche Google
                ajouter_a_memoire(msg, res)
                return ton_humain_reponse(f"Voici ce que j'ai trouv√© via Google :\n{res}")
        except Exception as e2:
            return ton_humain_reponse(f"Erreur lors de la recherche Google : {e2}")
        # En dernier recours, renvoie une erreur interne
        return ton_humain_reponse(f"Erreur interne lors de la g√©n√©ration de r√©ponse : {e}")

    # Si tout √©choue, r√©pond avec un message g√©n√©rique et ajoute en m√©moire
    ajouter_a_memoire(msg, "Je vais m'en souvenir pour la prochaine fois.")
    return ton_humain_reponse("Je ne connais pas encore la r√©ponse, mais je l'apprendrai pour toi !")
