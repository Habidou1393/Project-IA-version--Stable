import os  # Pour la gestion des fichiers
import torch  # PyTorch, framework ML
import torch.nn as nn  # Modules pour construire des r√©seaux de neurones
import torch.optim as optim  # Optimiseurs pour entra√Æner le mod√®le
from sentence_transformers import SentenceTransformer  # Pour g√©n√©rer des embeddings
from app.memory import memoire_cache, lock  # M√©moire partag√©e et verrou pour la synchronisation
import matplotlib.pyplot as plt  # Pour afficher les graphiques de loss

# Chargement du mod√®le d'embeddings multilingue
model = SentenceTransformer('distiluse-base-multilingual-cased-v1')
# D√©tecte si GPU est disponible sinon CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# D√©finition d'un bloc r√©siduel simple (inspiration Transformer)
class ResidualBlock(nn.Module):
    def __init__(self, dim):
        super().__init__()
        # Couche de normalisation + deux couches lin√©aires avec GELU et Dropout
        self.block = nn.Sequential(
            nn.LayerNorm(dim),
            nn.Linear(dim, dim),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(dim, dim),
        )

    def forward(self, x):
        # Ajoute la sortie du bloc au input (connexion r√©siduelle)
        return x + self.block(x)

# R√©seau de neurones complet avec attention multi-t√™tes et blocs r√©siduels
class ComplexNN(nn.Module):
    def __init__(self, input_dim=512, hidden_dim=1024, output_dim=512, num_blocks=4):
        super().__init__()
        # Couche lin√©aire d'entr√©e
        self.input_layer = nn.Linear(input_dim, hidden_dim)
        # Plusieurs blocs r√©siduels en s√©quence
        self.residual_blocks = nn.Sequential(*[ResidualBlock(hidden_dim) for _ in range(num_blocks)])
        # M√©canisme d'attention multi-t√™tes
        self.attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=8, batch_first=True)
        # Couche lin√©aire de sortie
        self.output_layer = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = self.input_layer(x)  # Passage de la couche d'entr√©e
        x = x.unsqueeze(1)  # Ajout d'une dimension batch sequence pour l'attention
        attn_output, _ = self.attention(x, x, x)  # Application de l'attention sur x
        x = self.residual_blocks(attn_output)  # Passage par les blocs r√©siduels
        x = x.squeeze(1)  # Suppression de la dimension sequence
        return self.output_layer(x)  # Passage par la couche de sortie

# Cr√©ation du mod√®le et transfert vers GPU si disponible
nn_model = ComplexNN().to(device)
# Optimiseur AdamW avec taux d'apprentissage et weight decay
optimizer = optim.AdamW(nn_model.parameters(), lr=1e-4, weight_decay=1e-5)
# Scheduler qui ajuste le learning rate de fa√ßon cosinuso√Ødale sur 10 epochs
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)
# Fonction de perte MSE (mean squared error) adapt√©e √† la r√©gression
loss_fn = nn.MSELoss()
# Gestionnaire d'√©chelle pour mixed precision (acc√©l√©ration GPU)
scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())

MODEL_PATH = "data/nn_model.pt"  # Chemin pour sauvegarder le mod√®le

# üíæ Fonction pour sauvegarder le mod√®le et l'optimiseur
def save_model():
    torch.save({
        'model_state_dict': nn_model.state_dict(),  # √âtat du mod√®le
        'optimizer_state_dict': optimizer.state_dict()  # √âtat de l'optimiseur
    }, MODEL_PATH)
    print(f"üíæ Mod√®le sauvegard√© dans {MODEL_PATH}")

# üì§ Fonction pour charger le mod√®le si le fichier existe
def load_model():
    if os.path.exists(MODEL_PATH):
        checkpoint = torch.load(MODEL_PATH, map_location=device)  # Chargement checkpoint
        nn_model.load_state_dict(checkpoint['model_state_dict'])  # Chargement poids mod√®le
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])  # Chargement optimiseur
        print(f"‚úÖ Mod√®le charg√© depuis {MODEL_PATH}")
    else:
        print("üÜï Aucun mod√®le trouv√©, nouveau mod√®le initialis√©.")

# üìà Fonction d'entra√Ænement du mod√®le sur la m√©moire
def train_nn_on_memory(epochs=10, show_plot=False):
    with lock:  # Bloc critique pour acc√©der √† la m√©moire partag√©e
        if len(memoire_cache) < 2:  # Pas assez d'exemples pour entra√Æner
            return
        questions = [item["question"] for item in memoire_cache]  # Extraction questions
        responses = [item["response"] for item in memoire_cache]  # Extraction r√©ponses

    with torch.no_grad():  # Pas de gradient pendant l'encodage embeddings
        # Encodage questions en tenseurs GPU
        q_embed = model.encode(questions, convert_to_tensor=True).to(device)
        # Encodage r√©ponses en tenseurs GPU
        r_embed = model.encode(responses, convert_to_tensor=True).to(device)

    nn_model.train()  # Mode entra√Ænement
    losses = []  # Liste pour stocker les pertes par epoch

    for epoch in range(epochs):
        optimizer.zero_grad()  # Remise √† z√©ro des gradients
        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):  # Mixed precision pour GPU
            pred = nn_model(q_embed)  # Pr√©dictions du mod√®le
            loss = loss_fn(pred, r_embed)  # Calcul de la perte MSE

        scaler.scale(loss).backward()  # R√©tropropagation avec √©chelle
        torch.nn.utils.clip_grad_norm_(nn_model.parameters(), max_norm=1.0)  # Clip gradients pour stabilit√©
        scaler.step(optimizer)  # Optimisation du mod√®le
        scaler.update()  # Mise √† jour du scaler
        scheduler.step()  # Mise √† jour du learning rate

        loss_value = loss.item()  # Extraction valeur scalaire de la perte
        losses.append(loss_value)  # Ajout dans l'historique
        print(f"Epoch {epoch+1}/{epochs} - Loss: {loss_value:.4f}")

    save_model()  # Sauvegarde du mod√®le apr√®s entra√Ænement

    if show_plot:  # Option pour afficher la courbe de perte
        plt.plot(range(1, epochs+1), losses, marker='o')
        plt.title("Courbe de perte")
        plt.xlabel("√âpochs")
        plt.ylabel("Loss")
        plt.grid()
        plt.show()

# üì• Ajouter une paire question/r√©ponse et entra√Æner rapidement
def ajouter_qa(question, reponse):
    with lock:  # Verrou pour modifier la m√©moire partag√©e
        memoire_cache.append({"question": question, "response": reponse})  # Ajout pair QA
    if len(memoire_cache) >= 2:
        train_nn_on_memory(epochs=2)  # Entra√Ænement rapide sur la m√©moire mise √† jour
